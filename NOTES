Our differentiator is how clever we can be about the variables we select. Everybody is going to be using the same learning methods.

Categorize products into types and predict a type for unseen products
Create dictionary of candidate variables. Seed it with the variables themselves. Add interaction terms motivated by what the Random Forest says is important.
Make a set of ensemble weights for each month?

Data Notes:
-Many categorical variables and some quantitative variables have 0 variance; remove these
-Some variables seem to be duplicates
-Auxiliary feature set to 0 for a present value and 1 for a 'NaN', so we can set all NaN's to zero safely
-Log transform of quantitative variables and sales. Is log appropriate for the quantitative features? Could also use splines.
-Naming of variables. There is both 'quant_22' and 'quan_22'
-Merge/collapse/condense the categorical variables?

Methods:
-GLM_Net: weighted combination of L1 and L2 ridge regressions
-Splines
-Logistic regression?
-Gaussian Processes
-Neural Networks; input these to other model to make model of residuals?
-Feature selection: BIC-score, scikit-learn
-Bagging: sample the data with replacement, train an ensemble on the sample, then average the predictions across models trained on the different samples
-Dimensionality reduction: LDA & SVD



